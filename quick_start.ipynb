{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2147,"status":"ok","timestamp":1678725825644,"user":{"displayName":"Glory Odeyemi","userId":"04491820538436953732"},"user_tz":240},"id":"v2RcazvZpGdB","outputId":"38a5d697-7ea9-4808-af21-4dfb8fcae53b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1678725831089,"user":{"displayName":"Glory Odeyemi","userId":"04491820538436953732"},"user_tz":240},"id":"6f05puUhpcFh","outputId":"ba037eae-32cd-4fb0-efc3-845a781166af"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive\n"]}],"source":["%cd drive/My Drive/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1147,"status":"ok","timestamp":1678725836251,"user":{"displayName":"Glory Odeyemi","userId":"04491820538436953732"},"user_tz":240},"id":"gpW89XyFpzeI","outputId":"cc870762-809d-4765-8780-543c314d59f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'COMP_8730_Project'...\n","remote: Enumerating objects: 148, done.\u001b[K\n","remote: Counting objects: 100% (148/148), done.\u001b[K\n","remote: Compressing objects: 100% (114/114), done.\u001b[K\n","remote: Total 148 (delta 73), reused 78 (delta 27), pack-reused 0\u001b[K\n","Receiving objects: 100% (148/148), 125.16 KiB | 4.47 MiB/s, done.\n","Resolving deltas: 100% (73/73), done.\n"]}],"source":["! git clone https://github.com/gloryodeyemi/COMP_8730_Project.git"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1678725853706,"user":{"displayName":"Glory Odeyemi","userId":"04491820538436953732"},"user_tz":240},"id":"n-TkIN3RtMvl","outputId":"4c0a0a05-af5f-43d4-815f-e0820724eddc"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/COMP_8730_Project\n"]}],"source":["%cd drive/My Drive/COMP_8730_Project"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7845,"status":"ok","timestamp":1678725864548,"user":{"displayName":"Glory Odeyemi","userId":"04491820538436953732"},"user_tz":240},"id":"sb5dEHXhqI9y"},"outputs":[],"source":["%%capture\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45824,"status":"ok","timestamp":1678725920114,"user":{"displayName":"Glory Odeyemi","userId":"04491820538436953732"},"user_tz":240},"id":"TH1rKZI2rE3h","outputId":"5969420b-bb96-401e-a589-cdf17758cb0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-13 16:44:36.169452: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-13 16:44:37.491815: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-13 16:44:37.491958: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-13 16:44:37.491979: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-88bea9a3b794c2af/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n","Downloading data files: 100% 1/1 [00:00<00:00, 2941.31it/s]\n","Extracting data files: 100% 1/1 [00:00<00:00, 144.81it/s]\n","Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-88bea9a3b794c2af/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n","100% 1/1 [00:00<00:00, 903.17it/s]\n","---------- Dataset ----------\n","DatasetDict({\n","    train: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary'],\n","        num_rows: 66\n","    })\n","})\n","---------- Updated dataset ----------\n","DatasetDict({\n","    train: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary'],\n","        num_rows: 41\n","    })\n","    test: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary'],\n","        num_rows: 14\n","    })\n","    validation: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary'],\n","        num_rows: 11\n","    })\n","})\n","---------- Example output ----------\n","{'Tweets': 'E jowo, mo fe ra phone charger, abi o le help me ni?', 'Eng_source': 'Please, I want to buy a phone charger, or can you help me?', 'Summary': 'I need to buy a phone charger; can you please assist me?'}\n","---------- Updated dataset ----------\n","DatasetDict({\n","    train: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language'],\n","        num_rows: 41\n","    })\n","    test: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language'],\n","        num_rows: 14\n","    })\n","    validation: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language'],\n","        num_rows: 11\n","    })\n","})\n","---------- Example output ----------\n","{'Tweets': 'E jowo, mo fe ra phone charger, abi o le help me ni?', 'Eng_source': 'Please, I want to buy a phone charger, or can you help me?', 'Summary': 'I need to buy a phone charger; can you please assist me?', 'Language': ['ENGLISH', None, 'YORUBA', 'ENGLISH', 'YORUBA', 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'ENGLISH', 'ENGLISH', None, None, None, 'YORUBA', 'YORUBA', None, 'ENGLISH', None, 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'YORUBA', None]}\n","---------- Updated dataset ----------\n","DatasetDict({\n","    train: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches'],\n","        num_rows: 41\n","    })\n","    test: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches'],\n","        num_rows: 14\n","    })\n","    validation: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches'],\n","        num_rows: 11\n","    })\n","})\n","---------- Example output ----------\n","{'Tweets': 'E jowo, mo fe ra phone charger, abi o le help me ni?', 'Eng_source': 'Please, I want to buy a phone charger, or can you help me?', 'Summary': 'I need to buy a phone charger; can you please assist me?', 'Language': ['ENGLISH', None, 'YORUBA', 'ENGLISH', 'YORUBA', 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'ENGLISH', 'ENGLISH', None, None, None, 'YORUBA', 'YORUBA', None, 'ENGLISH', None, 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'YORUBA', None], 'Code_switches': ['E', 'jowo', 'mo', 'fe', 'ra', 'phone', 'charger', 'abi', 'o', 'le', 'help', 'me', 'ni']}\n","Parameter 'function'=<function translate_tweet.<locals>.<lambda> at 0x7f95258f85e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","---------- Updated dataset ----------\n","DatasetDict({\n","    train: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet'],\n","        num_rows: 41\n","    })\n","    test: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet'],\n","        num_rows: 14\n","    })\n","    validation: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet'],\n","        num_rows: 11\n","    })\n","})\n","---------- Example output ----------\n","{'Tweets': 'E jowo, mo fe ra phone charger, abi o le help me ni?', 'Eng_source': 'Please, I want to buy a phone charger, or can you help me?', 'Summary': 'I need to buy a phone charger; can you please assist me?', 'Language': ['ENGLISH', None, 'YORUBA', 'ENGLISH', 'YORUBA', 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'ENGLISH', 'ENGLISH', None, None, None, 'YORUBA', 'YORUBA', None, 'ENGLISH', None, 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'YORUBA', None], 'Code_switches': ['E', 'jowo', 'mo', 'fe', 'ra', 'phone', 'charger', 'abi', 'o', 'le', 'help', 'me', 'ni'], 'Translated_tweet': 'Please, I want to buy a phone charger, can you help me?'}\n","---------- Updated dataset ----------\n","DatasetDict({\n","    train: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet', 'Bleu_score'],\n","        num_rows: 41\n","    })\n","    test: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet', 'Bleu_score'],\n","        num_rows: 14\n","    })\n","    validation: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet', 'Bleu_score'],\n","        num_rows: 11\n","    })\n","})\n","---------- Example output ----------\n","{'Tweets': 'E jowo, mo fe ra phone charger, abi o le help me ni?', 'Eng_source': 'Please, I want to buy a phone charger, or can you help me?', 'Summary': 'I need to buy a phone charger; can you please assist me?', 'Language': ['ENGLISH', None, 'YORUBA', 'ENGLISH', 'YORUBA', 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'ENGLISH', 'ENGLISH', None, None, None, 'YORUBA', 'YORUBA', None, 'ENGLISH', None, 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'YORUBA', None], 'Code_switches': ['E', 'jowo', 'mo', 'fe', 'ra', 'phone', 'charger', 'abi', 'o', 'le', 'help', 'me', 'ni'], 'Translated_tweet': 'Please, I want to buy a phone charger, can you help me?', 'Bleu_score': 0.7831566121841202}\n","Bleu score: 0.4990\n","Map:   0% 0/41 [00:00<?, ? examples/s]/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Language, Tweets, Bleu_score, Summary, Code_switches, Translated_tweet, Eng_source. If Language, Tweets, Bleu_score, Summary, Code_switches, Translated_tweet, Eng_source are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 41\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 30\n","  Number of trainable parameters = 139420416\n","  0% 0/30 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"," 33% 10/30 [00:04<00:07,  2.69it/s]The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Language, Tweets, Bleu_score, Summary, Code_switches, Translated_tweet, Eng_source. If Language, Tweets, Bleu_score, Summary, Code_switches, Translated_tweet, Eng_source are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 11\n","  Batch size = 2\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n","  0% 0/6 [00:00<?, ?it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n"," 33% 2/6 [00:00<00:00,  6.71it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n"," 50% 3/6 [00:00<00:00,  4.74it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n"," 67% 4/6 [00:00<00:00,  4.14it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n"," 83% 5/6 [00:01<00:00,  3.82it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n","                                   \n","\u001b[A{'eval_loss': 11.119074821472168, 'eval_rouge1': 0.46945524915926606, 'eval_rouge2': 0.2295482385064675, 'eval_rougeL': 0.4328121108994135, 'eval_rougeLsum': 0.44349437681760884, 'eval_runtime': 1.8025, 'eval_samples_per_second': 6.103, 'eval_steps_per_second': 3.329, 'epoch': 0.95}\n"," 33% 10/30 [00:06<00:07,  2.69it/s]\n","100% 6/6 [00:01<00:00,  4.43it/s]\u001b[A\n"," 67% 20/30 [00:09<00:03,  2.52it/s]The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Language, Tweets, Bleu_score, Summary, Code_switches, Translated_tweet, Eng_source. If Language, Tweets, Bleu_score, Summary, Code_switches, Translated_tweet, Eng_source are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 11\n","  Batch size = 2\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n","  0% 0/6 [00:00<?, ?it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n"," 33% 2/6 [00:00<00:00,  5.30it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n"," 50% 3/6 [00:00<00:00,  3.27it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n"," 67% 4/6 [00:01<00:00,  2.78it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n"," 83% 5/6 [00:01<00:00,  2.47it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n","                                   \n","\u001b[A{'eval_loss': 9.99132251739502, 'eval_rouge1': 0.46945524915926606, 'eval_rouge2': 0.2295482385064675, 'eval_rougeL': 0.4328121108994135, 'eval_rougeLsum': 0.44349437681760884, 'eval_runtime': 2.8063, 'eval_samples_per_second': 3.92, 'eval_steps_per_second': 2.138, 'epoch': 1.95}\n"," 67% 20/30 [00:12<00:03,  2.52it/s]\n","100% 6/6 [00:02<00:00,  2.74it/s]\u001b[A\n","100% 30/30 [00:16<00:00,  2.44it/s]The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Language, Tweets, Bleu_score, Summary, Code_switches, Translated_tweet, Eng_source. If Language, Tweets, Bleu_score, Summary, Code_switches, Translated_tweet, Eng_source are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 11\n","  Batch size = 2\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n","  0% 0/6 [00:00<?, ?it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n"," 33% 2/6 [00:00<00:00,  5.12it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n"," 50% 3/6 [00:00<00:00,  3.92it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n"," 67% 4/6 [00:01<00:00,  3.38it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n"," 83% 5/6 [00:01<00:00,  3.05it/s]\u001b[AGenerate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","\n","                                   \n","\u001b[A{'eval_loss': 9.450173377990723, 'eval_rouge1': 0.46833359445135725, 'eval_rouge2': 0.22929033549929762, 'eval_rougeL': 0.4315314295599601, 'eval_rougeLsum': 0.44169030050078883, 'eval_runtime': 2.2878, 'eval_samples_per_second': 4.808, 'eval_steps_per_second': 2.623, 'epoch': 2.95}\n","100% 30/30 [00:18<00:00,  2.44it/s]\n","100% 6/6 [00:01<00:00,  3.58it/s]\u001b[A\n","                                 \u001b[A\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 18.8607, 'train_samples_per_second': 6.521, 'train_steps_per_second': 1.591, 'train_loss': 11.0942138671875, 'epoch': 2.95}\n","100% 30/30 [00:18<00:00,  1.59it/s]\n","***** Running Prediction *****\n","  Num examples = 1\n","  Batch size = 2\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","100% 1/1 [00:00<00:00, 2681.78it/s]\n","Original tweet:  I hope say you dey alright, mo n'ife re gan ni.\n","Translated tweet:  I hope you say you're alright, I really love you.\n","Generated summary:  </s><s>I hope you say you're alright, I really love you.</s><pad><pad><pad><pad>\n"]}],"source":["!python main.py"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyOXnM90NZq+VXlq6cq1HeTJ"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}