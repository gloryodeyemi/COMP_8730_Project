{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27018,"status":"ok","timestamp":1679328756919,"user":{"displayName":"Glory Odeyemi","userId":"04491820538436953732"},"user_tz":240},"id":"v2RcazvZpGdB","outputId":"f6c547dd-65b1-41f0-caf1-ab75ac902aef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1050,"status":"ok","timestamp":1679328765748,"user":{"displayName":"Glory Odeyemi","userId":"04491820538436953732"},"user_tz":240},"id":"6f05puUhpcFh","outputId":"7b9b4818-19b6-4fd1-d63b-e4fb1a06811a"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive\n"]}],"source":["%cd drive/My Drive/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2626,"status":"ok","timestamp":1679328773615,"user":{"displayName":"Glory Odeyemi","userId":"04491820538436953732"},"user_tz":240},"id":"gpW89XyFpzeI","outputId":"2cc05696-95a1-413c-b369-e3289f075a80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'COMP_8730_Project'...\n","remote: Enumerating objects: 207, done.\u001b[K\n","remote: Counting objects: 100% (207/207), done.\u001b[K\n","remote: Compressing objects: 100% (153/153), done.\u001b[K\n","remote: Total 207 (delta 113), reused 111 (delta 47), pack-reused 0\u001b[K\n","Receiving objects: 100% (207/207), 179.55 KiB | 421.00 KiB/s, done.\n","Resolving deltas: 100% (113/113), done.\n"]}],"source":["! git clone https://github.com/gloryodeyemi/COMP_8730_Project.git"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1679328793399,"user":{"displayName":"Glory Odeyemi","userId":"04491820538436953732"},"user_tz":240},"id":"n-TkIN3RtMvl","outputId":"852c93d4-8596-4938-8fd5-351d10c28b7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/COMP_8730_Project\n"]}],"source":["%cd drive/My Drive/COMP_8730_Project"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":40775,"status":"ok","timestamp":1679328838979,"user":{"displayName":"Glory Odeyemi","userId":"04491820538436953732"},"user_tz":240},"id":"sb5dEHXhqI9y"},"outputs":[],"source":["%%capture\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89550,"status":"ok","timestamp":1679328933753,"user":{"displayName":"Glory Odeyemi","userId":"04491820538436953732"},"user_tz":240},"id":"TH1rKZI2rE3h","outputId":"78c9f60f-ff4b-42a4-cbc0-5c17c81fba7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-20 16:14:09.497704: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-20 16:14:12.251203: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-20 16:14:12.251359: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-20 16:14:12.251385: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Downloading (…)lve/main/config.json: 100% 1.72k/1.72k [00:00<00:00, 268kB/s]\n","Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:01<00:00, 808kB/s]\n","Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 511kB/s]\n","Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.21MB/s]\n","Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 4.87MB/s]\n","Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-7330486750bfe3be/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n","Downloading data files: 100% 1/1 [00:00<00:00, 3342.07it/s]\n","Extracting data files: 100% 1/1 [00:00<00:00, 157.31it/s]\n","Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-7330486750bfe3be/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n","100% 1/1 [00:00<00:00, 781.64it/s]\n","---------- Dataset ----------\n","DatasetDict({\n","    train: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary'],\n","        num_rows: 100\n","    })\n","})\n","---------- Updated dataset ----------\n","DatasetDict({\n","    train: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary'],\n","        num_rows: 64\n","    })\n","    test: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary'],\n","        num_rows: 20\n","    })\n","    validation: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary'],\n","        num_rows: 16\n","    })\n","})\n","---------- Example output ----------\n","{'Tweets': 'Happy birthday to my darling Adebisi! Ojo ayo yi ni yi o, mo wa dupe lowo Oluwa.', 'Eng_source': \"Happy birthday to my darling Adebisi! This is a day of joy, and I'm grateful to God.\", 'Summary': \"Happy Birthday Adebisi! I'm grateful to God for this joyful day.\"}\n","---------- Updated dataset ----------\n","DatasetDict({\n","    train: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language'],\n","        num_rows: 64\n","    })\n","    test: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language'],\n","        num_rows: 20\n","    })\n","    validation: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language'],\n","        num_rows: 16\n","    })\n","})\n","---------- Example output ----------\n","{'Tweets': 'Happy birthday to my darling Adebisi! Ojo ayo yi ni yi o, mo wa dupe lowo Oluwa.', 'Eng_source': \"Happy birthday to my darling Adebisi! This is a day of joy, and I'm grateful to God.\", 'Summary': \"Happy Birthday Adebisi! I'm grateful to God for this joyful day.\", 'Language': ['ENGLISH', None, 'ENGLISH', 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'YORUBA', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'YORUBA', 'YORUBA', 'YORUBA', None, None, 'ENGLISH', 'ENGLISH', 'YORUBA', 'YORUBA', 'ENGLISH', 'YORUBA', None, None, 'ENGLISH', 'YORUBA', 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'YORUBA', None, 'YORUBA', 'YORUBA', None, 'ENGLISH', 'YORUBA', None, 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'YORUBA', None, None, 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'ENGLISH', 'YORUBA', None, None]}\n","---------- Updated dataset ----------\n","DatasetDict({\n","    train: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches'],\n","        num_rows: 64\n","    })\n","    test: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches'],\n","        num_rows: 20\n","    })\n","    validation: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches'],\n","        num_rows: 16\n","    })\n","})\n","---------- Example output ----------\n","{'Tweets': 'Happy birthday to my darling Adebisi! Ojo ayo yi ni yi o, mo wa dupe lowo Oluwa.', 'Eng_source': \"Happy birthday to my darling Adebisi! This is a day of joy, and I'm grateful to God.\", 'Summary': \"Happy Birthday Adebisi! I'm grateful to God for this joyful day.\", 'Language': ['ENGLISH', None, 'ENGLISH', 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'YORUBA', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'YORUBA', 'YORUBA', 'YORUBA', None, None, 'ENGLISH', 'ENGLISH', 'YORUBA', 'YORUBA', 'ENGLISH', 'YORUBA', None, None, 'ENGLISH', 'YORUBA', 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'YORUBA', None, 'YORUBA', 'YORUBA', None, 'ENGLISH', 'YORUBA', None, 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'YORUBA', None, None, 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'ENGLISH', 'YORUBA', None, None], 'Code_switches': [['Happy', 'birthday'], ['birthday', 'to'], ['darling', 'Adebisi!'], ['Adebisi!', 'Ojo'], ['Ojo', 'ayo'], ['ayo', 'yi'], ['yi', 'o,'], ['o,', 'mo'], ['lowo', 'Oluwa.']]}\n","Parameter 'function'=<function translate_tweet.<locals>.<lambda> at 0x7f1e1843eca0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","---------- Updated dataset ----------\n","DatasetDict({\n","    train: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet'],\n","        num_rows: 64\n","    })\n","    test: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet'],\n","        num_rows: 20\n","    })\n","    validation: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet'],\n","        num_rows: 16\n","    })\n","})\n","---------- Example output ----------\n","{'Tweets': 'Happy birthday to my darling Adebisi! Ojo ayo yi ni yi o, mo wa dupe lowo Oluwa.', 'Eng_source': \"Happy birthday to my darling Adebisi! This is a day of joy, and I'm grateful to God.\", 'Summary': \"Happy Birthday Adebisi! I'm grateful to God for this joyful day.\", 'Language': ['ENGLISH', None, 'ENGLISH', 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'YORUBA', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'YORUBA', 'YORUBA', 'YORUBA', None, None, 'ENGLISH', 'ENGLISH', 'YORUBA', 'YORUBA', 'ENGLISH', 'YORUBA', None, None, 'ENGLISH', 'YORUBA', 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'YORUBA', None, 'YORUBA', 'YORUBA', None, 'ENGLISH', 'YORUBA', None, 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'YORUBA', None, None, 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'ENGLISH', 'YORUBA', None, None], 'Code_switches': [['Happy', 'birthday'], ['birthday', 'to'], ['darling', 'Adebisi!'], ['Adebisi!', 'Ojo'], ['Ojo', 'ayo'], ['ayo', 'yi'], ['yi', 'o,'], ['o,', 'mo'], ['lowo', 'Oluwa.']], 'Translated_tweet': 'Happy birthday to my darling Adebisi! This is a happy day, I am grateful to the Lord.'}\n","---------- Updated dataset ----------\n","DatasetDict({\n","    train: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet', 'Bleu_score'],\n","        num_rows: 64\n","    })\n","    test: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet', 'Bleu_score'],\n","        num_rows: 20\n","    })\n","    validation: Dataset({\n","        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet', 'Bleu_score'],\n","        num_rows: 16\n","    })\n","})\n","---------- Example output ----------\n","{'Tweets': 'Happy birthday to my darling Adebisi! Ojo ayo yi ni yi o, mo wa dupe lowo Oluwa.', 'Eng_source': \"Happy birthday to my darling Adebisi! This is a day of joy, and I'm grateful to God.\", 'Summary': \"Happy Birthday Adebisi! I'm grateful to God for this joyful day.\", 'Language': ['ENGLISH', None, 'ENGLISH', 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'YORUBA', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'YORUBA', 'YORUBA', 'YORUBA', None, None, 'ENGLISH', 'ENGLISH', 'YORUBA', 'YORUBA', 'ENGLISH', 'YORUBA', None, None, 'ENGLISH', 'YORUBA', 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'YORUBA', None, 'YORUBA', 'YORUBA', None, 'ENGLISH', 'YORUBA', None, 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'YORUBA', None, None, 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'ENGLISH', 'YORUBA', None, None], 'Code_switches': [['Happy', 'birthday'], ['birthday', 'to'], ['darling', 'Adebisi!'], ['Adebisi!', 'Ojo'], ['Ojo', 'ayo'], ['ayo', 'yi'], ['yi', 'o,'], ['o,', 'mo'], ['lowo', 'Oluwa.']], 'Translated_tweet': 'Happy birthday to my darling Adebisi! This is a happy day, I am grateful to the Lord.', 'Bleu_score': 0.5459060494040118}\n","Bleu score: 0.5441\n","Map:   0% 0/64 [00:00<?, ? examples/s]/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:3586: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","Downloading pytorch_model.bin: 100% 558M/558M [00:01<00:00, 302MB/s]\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","  0% 0/48 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"," 33% 16/48 [00:09<00:12,  2.63it/s]\n","  0% 0/8 [00:00<?, ?it/s]\u001b[A\n"," 25% 2/8 [00:00<00:00,  6.23it/s]\u001b[A\n"," 38% 3/8 [00:00<00:01,  3.87it/s]\u001b[A\n"," 50% 4/8 [00:01<00:01,  3.19it/s]\u001b[A\n"," 62% 5/8 [00:01<00:01,  2.86it/s]\u001b[A\n"," 75% 6/8 [00:01<00:00,  2.72it/s]\u001b[A\n"," 88% 7/8 [00:02<00:00,  2.61it/s]\u001b[A\n","100% 8/8 [00:02<00:00,  2.81it/s]\u001b[A\n","{'eval_loss': 9.701282501220703, 'eval_rouge1': 0.5232592670179261, 'eval_rouge2': 0.3196998998769299, 'eval_rougeL': 0.4880834378780986, 'eval_rougeLsum': 0.49156944456385093, 'eval_runtime': 3.2394, 'eval_samples_per_second': 4.939, 'eval_steps_per_second': 2.47, 'epoch': 1.0}\n","\n"," 33% 16/48 [00:12<00:12,  2.63it/s]\n"," 67% 32/48 [00:18<00:06,  2.65it/s]\n","  0% 0/8 [00:00<?, ?it/s]\u001b[A\n"," 25% 2/8 [00:00<00:00,  7.79it/s]\u001b[A\n"," 38% 3/8 [00:00<00:00,  5.14it/s]\u001b[A\n"," 50% 4/8 [00:00<00:00,  4.34it/s]\u001b[A\n"," 62% 5/8 [00:01<00:00,  4.11it/s]\u001b[A\n"," 75% 6/8 [00:01<00:00,  3.82it/s]\u001b[A\n"," 88% 7/8 [00:01<00:00,  3.70it/s]\u001b[A\n","100% 8/8 [00:01<00:00,  3.64it/s]\u001b[A\n","{'eval_loss': 7.255222797393799, 'eval_rouge1': 0.5234311712045383, 'eval_rouge2': 0.31461548563771813, 'eval_rougeL': 0.491127204136503, 'eval_rougeLsum': 0.4928073934268469, 'eval_runtime': 2.4146, 'eval_samples_per_second': 6.626, 'eval_steps_per_second': 3.313, 'epoch': 2.0}\n","\n"," 67% 32/48 [00:20<00:06,  2.65it/s]\n","100% 48/48 [00:26<00:00,  2.57it/s]\n","  0% 0/8 [00:00<?, ?it/s]\u001b[A\n"," 25% 2/8 [00:00<00:00,  7.73it/s]\u001b[A\n"," 38% 3/8 [00:00<00:00,  5.05it/s]\u001b[A\n"," 50% 4/8 [00:00<00:00,  4.29it/s]\u001b[A\n"," 62% 5/8 [00:01<00:00,  4.11it/s]\u001b[A\n"," 75% 6/8 [00:01<00:00,  3.83it/s]\u001b[A\n"," 88% 7/8 [00:01<00:00,  3.69it/s]\u001b[A\n","100% 8/8 [00:01<00:00,  3.66it/s]\u001b[A\n","{'eval_loss': 5.96372652053833, 'eval_rouge1': 0.5230577552594091, 'eval_rouge2': 0.311589793787298, 'eval_rougeL': 0.48676747842315615, 'eval_rougeLsum': 0.4881697326453135, 'eval_runtime': 2.5424, 'eval_samples_per_second': 6.293, 'eval_steps_per_second': 3.147, 'epoch': 3.0}\n","\n","100% 48/48 [00:29<00:00,  2.57it/s]\n","{'train_runtime': 29.3674, 'train_samples_per_second': 6.538, 'train_steps_per_second': 1.634, 'train_loss': 9.15548578898112, 'epoch': 3.0}\n","100% 48/48 [00:29<00:00,  1.64it/s]\n","100% 8/8 [00:02<00:00,  3.73it/s]\n","100% 1/1 [00:00<00:00, 3048.19it/s]\n","Original tweet:  My mate has an offer as a junior project manager at his company, Ta lo fe?\n","Translated tweet:  My mate has an offer as a junior project manager at his company, who wants it?\n","Generated summary:  </s><s>My mate has an offer as a junior project manager at his company, who wants it</s>\n"]}],"source":["!python main.py"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyOXnM90NZq+VXlq6cq1HeTJ"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}