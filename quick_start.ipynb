{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17407,
     "status": "ok",
     "timestamp": 1677440372448,
     "user": {
      "displayName": "Glory Odeyemi",
      "userId": "04491820538436953732"
     },
     "user_tz": 300
    },
    "id": "v2RcazvZpGdB",
    "outputId": "a8a42c25-8ad8-408a-d05b-c734710e5ee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1677440392942,
     "user": {
      "displayName": "Glory Odeyemi",
      "userId": "04491820538436953732"
     },
     "user_tz": 300
    },
    "id": "6f05puUhpcFh",
    "outputId": "69ca57b6-f876-44b9-f000-510576f0dfa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive\n"
     ]
    }
   ],
   "source": [
    "%cd drive/My Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1201,
     "status": "ok",
     "timestamp": 1677440400382,
     "user": {
      "displayName": "Glory Odeyemi",
      "userId": "04491820538436953732"
     },
     "user_tz": 300
    },
    "id": "gpW89XyFpzeI",
    "outputId": "1ee38592-08e0-4aa9-e037-4c94a0a6808f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'COMP_8730_Project'...\n",
      "remote: Enumerating objects: 110, done.\u001b[K\n",
      "remote: Counting objects: 100% (110/110), done.\u001b[K\n",
      "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
      "remote: Total 110 (delta 53), reused 61 (delta 21), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (110/110), 80.81 KiB | 3.51 MiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/gloryodeyemi/COMP_8730_Project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1677440509597,
     "user": {
      "displayName": "Glory Odeyemi",
      "userId": "04491820538436953732"
     },
     "user_tz": 300
    },
    "id": "n-TkIN3RtMvl",
    "outputId": "970f202c-5c74-4816-a157-a7b4cc415fbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/COMP_8730_Project\n"
     ]
    }
   ],
   "source": [
    "%cd drive/My Drive/COMP_8730_Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36094,
     "status": "ok",
     "timestamp": 1677440552322,
     "user": {
      "displayName": "Glory Odeyemi",
      "userId": "04491820538436953732"
     },
     "user_tz": 300
    },
    "id": "sb5dEHXhqI9y",
    "outputId": "4ec7e8ef-1ade-462d-eec1-33ecec6e6fbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (2.11.0)\n",
      "Collecting lingua-language-detector\n",
      "  Downloading lingua_language_detector-1.3.2-py3-none-any.whl (86.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (3.5.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (3.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (1.22.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (1.3.5)\n",
      "Requirement already satisfied: scikit_learn in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (1.0.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (0.11.2)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (2.11.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.10.0-py3-none-any.whl (469 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (0.14.1+cu116)\n",
      "Collecting googletrans==3.1.0a0\n",
      "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting httpx==0.13.3\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.8/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0->-r requirements.txt (line 16)) (2.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0->-r requirements.txt (line 16)) (2022.12.7)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting chardet==3.*\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hstspreload\n",
      "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpcore==0.9.*\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.10,>=0.8\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h2==3.*\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex<2023.0.0,>=2022.10.31\n",
      "  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 KB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 3)) (7.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 3)) (23.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->-r requirements.txt (line 4)) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk->-r requirements.txt (line 4)) (4.64.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 6)) (2022.7.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn->-r requirements.txt (line 7)) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn->-r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (1.14.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (0.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (4.5.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (2.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (1.51.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (1.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (23.1.21)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (3.19.6)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (2.11.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (57.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (0.30.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (3.1.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 9)) (15.0.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers->-r requirements.txt (line 10)) (2.25.1)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers->-r requirements.txt (line 10)) (3.9.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers->-r requirements.txt (line 10)) (6.0)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->-r requirements.txt (line 12)) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->-r requirements.txt (line 12)) (9.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->-r requirements.txt (line 12)) (2023.1.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets->-r requirements.txt (line 12)) (0.3.6)\n",
      "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchvision->-r requirements.txt (line 15)) (1.13.1+cu116)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 9)) (0.38.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r requirements.txt (line 12)) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r requirements.txt (line 12)) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r requirements.txt (line 12)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r requirements.txt (line 12)) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r requirements.txt (line 12)) (3.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r requirements.txt (line 12)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->-r requirements.txt (line 12)) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->-r requirements.txt (line 10)) (1.24.3)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.1.0\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (2.16.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (1.8.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (3.14.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 9)) (3.2.2)\n",
      "Building wheels for collected packages: googletrans, rouge_score\n",
      "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16368 sha256=a241a92ff393e32fa225f946c1c8edcdf690ee07d39429675e6e257c131aabae\n",
      "  Stored in directory: /root/.cache/pip/wheels/dd/59/af/8d6c96a719763990f1c548e36b17d9efdfb767f42f7ff39f53\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=8d56e99a66cb82837ff4e399a718e263e8c1ee39c9615d3de660d9a9dcc4b821\n",
      "  Stored in directory: /root/.cache/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\n",
      "Successfully built googletrans rouge_score\n",
      "Installing collected packages: tokenizers, sentencepiece, rfc3986, hyperframe, hpack, h11, chardet, xxhash, urllib3, sniffio, regex, numpy, multiprocess, hstspreload, h2, scipy, lingua-language-detector, httpcore, rouge_score, responses, huggingface-hub, httpx, transformers, googletrans, datasets, evaluate\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2022.6.2\n",
      "    Uninstalling regex-2022.6.2:\n",
      "      Successfully uninstalled regex-2022.6.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.3\n",
      "    Uninstalling scipy-1.7.3:\n",
      "      Successfully uninstalled scipy-1.7.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chardet-3.0.4 datasets-2.10.0 evaluate-0.4.0 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 huggingface-hub-0.12.1 hyperframe-5.2.0 lingua-language-detector-1.3.2 multiprocess-0.70.14 numpy-1.24.2 regex-2022.10.31 responses-0.18.0 rfc3986-1.5.0 rouge_score-0.1.2 scipy-1.10.1 sentencepiece-0.1.97 sniffio-1.3.0 tokenizers-0.13.2 transformers-4.26.1 urllib3-1.26.14 xxhash-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65156,
     "status": "ok",
     "timestamp": 1677440704416,
     "user": {
      "displayName": "Glory Odeyemi",
      "userId": "04491820538436953732"
     },
     "user_tz": 300
    },
    "id": "TH1rKZI2rE3h",
    "outputId": "c379067a-1744-4e03-c471-ee125c2d294f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-26 19:43:59.789037: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-26 19:44:01.090888: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-02-26 19:44:01.091019: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-02-26 19:44:01.091044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "---------- Downloading punkt package ----------\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "\n",
      "---------- Loading the dataset ----------\n",
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-bcf6d52dcd1045e1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
      "Downloading data files: 100% 1/1 [00:00<00:00, 3034.95it/s]\n",
      "Extracting data files: 100% 1/1 [00:00<00:00, 138.13it/s]\n",
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-bcf6d52dcd1045e1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
      "100% 1/1 [00:00<00:00, 657.21it/s]\n",
      "\n",
      "---------- Dataset ----------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "})\n",
      "\n",
      "---------- Splitting the dataset into train, test, and validation ----------\n",
      "\n",
      "---------- Updated dataset ----------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary'],\n",
      "        num_rows: 53\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary'],\n",
      "        num_rows: 7\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary'],\n",
      "        num_rows: 6\n",
      "    })\n",
      "})\n",
      "---------- Example output ----------\n",
      "{'Tweets': 'E jowo, mo fe ra phone charger, abi o le help me ni?', 'Eng_source': 'Please, I want to buy a phone charger, or can you help me?', 'Summary': 'I need to buy a phone charger; can you please assist me?'}\n",
      "\n",
      "---------- Identifying the language ----------\n",
      "\n",
      "---------- Updated dataset ----------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary', 'Language'],\n",
      "        num_rows: 53\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary', 'Language'],\n",
      "        num_rows: 7\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary', 'Language'],\n",
      "        num_rows: 6\n",
      "    })\n",
      "})\n",
      "---------- Example output ----------\n",
      "{'Tweets': 'E jowo, mo fe ra phone charger, abi o le help me ni?', 'Eng_source': 'Please, I want to buy a phone charger, or can you help me?', 'Summary': 'I need to buy a phone charger; can you please assist me?', 'Language': ['ENGLISH', None, 'YORUBA', 'ENGLISH', 'YORUBA', 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'ENGLISH', 'ENGLISH', None, None, None, 'YORUBA', 'YORUBA', None, 'ENGLISH', None, 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'YORUBA', None]}\n",
      "\n",
      "---------- Code-switching detection ----------\n",
      "\n",
      "---------- Updatad dataset ----------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches'],\n",
      "        num_rows: 53\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches'],\n",
      "        num_rows: 7\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches'],\n",
      "        num_rows: 6\n",
      "    })\n",
      "})\n",
      "---------- Example output ----------\n",
      "{'Tweets': 'E jowo, mo fe ra phone charger, abi o le help me ni?', 'Eng_source': 'Please, I want to buy a phone charger, or can you help me?', 'Summary': 'I need to buy a phone charger; can you please assist me?', 'Language': ['ENGLISH', None, 'YORUBA', 'ENGLISH', 'YORUBA', 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'ENGLISH', 'ENGLISH', None, None, None, 'YORUBA', 'YORUBA', None, 'ENGLISH', None, 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'YORUBA', None], 'Code_switches': ['E', 'jowo', 'mo', 'fe', 'ra', 'phone', 'charger', 'abi', 'o', 'le', 'help', 'me', 'ni']}\n",
      "\n",
      "---------- Translation ----------\n",
      "\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function <lambda> at 0x7f375a54e670> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "---------- Updatad dataset ----------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet'],\n",
      "        num_rows: 53\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet'],\n",
      "        num_rows: 7\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet'],\n",
      "        num_rows: 6\n",
      "    })\n",
      "})\n",
      "---------- Example output ----------\n",
      "{'Tweets': 'E jowo, mo fe ra phone charger, abi o le help me ni?', 'Eng_source': 'Please, I want to buy a phone charger, or can you help me?', 'Summary': 'I need to buy a phone charger; can you please assist me?', 'Language': ['ENGLISH', None, 'YORUBA', 'ENGLISH', 'YORUBA', 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'ENGLISH', 'ENGLISH', None, None, None, 'YORUBA', 'YORUBA', None, 'ENGLISH', None, 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'YORUBA', None], 'Code_switches': ['E', 'jowo', 'mo', 'fe', 'ra', 'phone', 'charger', 'abi', 'o', 'le', 'help', 'me', 'ni'], 'Translated_tweet': 'Please, I want to buy a phone charger, can you help me?'}\n",
      "\n",
      "---------- Computing BLEU score ----------\n",
      "\n",
      "---------- Updatad dataset ----------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet', 'Bleu_score'],\n",
      "        num_rows: 53\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet', 'Bleu_score'],\n",
      "        num_rows: 7\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Tweets', 'Eng_source', 'Summary', 'Language', 'Code_switches', 'Translated_tweet', 'Bleu_score'],\n",
      "        num_rows: 6\n",
      "    })\n",
      "})\n",
      "---------- Example output ----------\n",
      "{'Tweets': 'E jowo, mo fe ra phone charger, abi o le help me ni?', 'Eng_source': 'Please, I want to buy a phone charger, or can you help me?', 'Summary': 'I need to buy a phone charger; can you please assist me?', 'Language': ['ENGLISH', None, 'YORUBA', 'ENGLISH', 'YORUBA', 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', None, None, 'ENGLISH', 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'ENGLISH', 'YORUBA', 'ENGLISH', 'ENGLISH', None, None, None, 'YORUBA', 'YORUBA', None, 'ENGLISH', None, 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', 'YORUBA', 'ENGLISH', None, 'ENGLISH', 'ENGLISH', None, 'YORUBA', 'YORUBA', None], 'Code_switches': ['E', 'jowo', 'mo', 'fe', 'ra', 'phone', 'charger', 'abi', 'o', 'le', 'help', 'me', 'ni'], 'Translated_tweet': 'Please, I want to buy a phone charger, can you help me?', 'Bleu_score': 0.7831566121841202}\n",
      "\n",
      "---------- BLEU score ----------\n",
      "Bleu score: 0.4926\n",
      "\n",
      "---------- Summarization ----------\n",
      "\n",
      "\n",
      "Downloading (…)lve/main/config.json: 100% 1.72k/1.72k [00:00<00:00, 266kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 5.46MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 3.84MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 9.47MB/s]\n",
      "\n",
      "---------- Data preprocessing ----------\n",
      "Map:   0% 0/53 [00:00<?, ? examples/s]/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                     \n",
      "---------- Loading the model ----------\n",
      "Downloading (…)\"pytorch_model.bin\";: 100% 558M/558M [00:08<00:00, 63.6MB/s]\n",
      "\n",
      "---------- Data collator ----------\n",
      "\n",
      "---------- Loading the ROUGE metric ----------\n",
      "Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 6.30MB/s]\n",
      "\n",
      "---------- Fine-tuning the model ----------\n",
      "\n",
      "---------- Training the model ----------\n",
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Bleu_score, Tweets, Code_switches, Translated_tweet, Language, Eng_source, Summary. If Bleu_score, Tweets, Code_switches, Translated_tweet, Language, Eng_source, Summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 53\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 39\n",
      "  Number of trainable parameters = 139420416\n",
      "  0% 0/39 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33% 13/39 [00:07<00:10,  2.52it/s]The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Bleu_score, Tweets, Code_switches, Translated_tweet, Language, Eng_source, Summary. If Bleu_score, Tweets, Code_switches, Translated_tweet, Language, Eng_source, Summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 2\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "\n",
      "  0% 0/3 [00:00<?, ?it/s]\u001b[AGenerate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "\n",
      " 67% 2/3 [00:00<00:00,  4.96it/s]\u001b[AGenerate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "\n",
      "                                   \n",
      "\u001b[A{'eval_loss': 10.402865409851074, 'eval_rouge1': 0.4918316753588072, 'eval_rouge2': 0.2680944221346538, 'eval_rougeL': 0.46270346076547625, 'eval_rougeLsum': 0.4758118805706293, 'eval_runtime': 1.5699, 'eval_samples_per_second': 3.822, 'eval_steps_per_second': 1.911, 'epoch': 0.96}\n",
      " 33% 13/39 [00:09<00:10,  2.52it/s]\n",
      "100% 3/3 [00:01<00:00,  3.47it/s]\u001b[A\n",
      " 67% 26/39 [00:14<00:05,  2.60it/s]The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Bleu_score, Tweets, Code_switches, Translated_tweet, Language, Eng_source, Summary. If Bleu_score, Tweets, Code_switches, Translated_tweet, Language, Eng_source, Summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 2\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "\n",
      "  0% 0/3 [00:00<?, ?it/s]\u001b[AGenerate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "\n",
      " 67% 2/3 [00:00<00:00,  6.80it/s]\u001b[AGenerate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "\n",
      "                                   \n",
      "\u001b[A{'eval_loss': 8.954598426818848, 'eval_rouge1': 0.4918316753588072, 'eval_rouge2': 0.2680944221346538, 'eval_rougeL': 0.46270346076547625, 'eval_rougeLsum': 0.4758118805706293, 'eval_runtime': 1.0376, 'eval_samples_per_second': 5.782, 'eval_steps_per_second': 2.891, 'epoch': 1.96}\n",
      " 67% 26/39 [00:15<00:05,  2.60it/s]\n",
      "100% 3/3 [00:00<00:00,  4.75it/s]\u001b[A\n",
      "100% 39/39 [00:20<00:00,  2.54it/s]The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: Bleu_score, Tweets, Code_switches, Translated_tweet, Language, Eng_source, Summary. If Bleu_score, Tweets, Code_switches, Translated_tweet, Language, Eng_source, Summary are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 2\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "\n",
      "  0% 0/3 [00:00<?, ?it/s]\u001b[AGenerate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "\n",
      " 67% 2/3 [00:00<00:00,  5.18it/s]\u001b[AGenerate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "\n",
      "                                   \n",
      "\u001b[A{'eval_loss': 8.239453315734863, 'eval_rouge1': 0.4918316753588072, 'eval_rouge2': 0.2680944221346538, 'eval_rougeL': 0.46270346076547625, 'eval_rougeLsum': 0.4758118805706293, 'eval_runtime': 1.3969, 'eval_samples_per_second': 4.295, 'eval_steps_per_second': 2.148, 'epoch': 2.96}\n",
      "100% 39/39 [00:21<00:00,  2.54it/s]\n",
      "100% 3/3 [00:01<00:00,  3.71it/s]\u001b[A\n",
      "                                 \u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 21.7418, 'train_samples_per_second': 7.313, 'train_steps_per_second': 1.794, 'train_loss': 10.66672613681891, 'epoch': 2.96}\n",
      "100% 39/39 [00:21<00:00,  1.80it/s]\n",
      "\n",
      "---------- Testing the model ----------\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 2\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "100% 1/1 [00:00<00:00, 2581.11it/s]\n",
      "Original tweet:  Ah, o ti o! I don forget say I suppose call my mama today o.\n",
      "Translated tweet:  Ah, that's it! I don't forget to say I suppose call my mom today.\n",
      "Generated summary:  </s><s>Ah, that's it! I don't forget to say I suppose call my mom</s>\n"
     ]
    }
   ],
   "source": [
    "!python Summarization.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMTFDgpfQAigkzDKKtTUTSh",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
